# Repository Description
This repository contains a comprehensive analysis of the effects of text watermarking on the quality and classification accuracy of text generated by language models. The study focuses on evaluating how a watermarking algorithm, as described in the paper [A Watermark for Large Language Models](https://arxiv.org/pdf/2301.10226) impacts key metrics such as BLEU, ROUGE, and METEOR scores, as well as text classification accuracy using T5 and Flan-T5 models.


# Setup Instructions

## 1.Clone the Repository:
```bash
git clone https://github.com/youssefkhalil320/Text-Watermarking-Analysis.git
cd Text-Watermarking-Analysis
```

## 2.Create and Activate a Conda Environment:
```bash
conda create -n watermark_analysis python=3.9
conda activate watermark_analysis
```

## 3.Navigate to the Project Directory:
```bash
cd project
```

## 4.Install Required Python Packages:
```bash
pip install -r requirements.txt
```

## Repository Structure

The repository is organized into several key directories:

- **data/**: Contains datasets used for analysis, including both watermarked and non-watermarked versions of text generated by T5 and Flan-T5 models on the CNN/DailyMail and XSum datasets.

- **helper_scripts/**: Includes scripts for converting datasets from Hugging Face format to CSV, making it easier to work with the data in various analysis scripts.

- **modules/**: Contains scripts for calculating BLEU, ROUGE, and METEOR scores, which are essential for assessing the quality of text after watermarking.

- **plots/**: Stores the visualizations of the metrics calculated during the analysis, helping to illustrate the impact of watermarking on text quality.

- **text_classification/**: Includes scripts and notebooks for evaluating text classification accuracy before and after watermarking, allowing for a deeper understanding of how watermarking affects downstream tasks.

- **text_watermarking/**: Contains notebooks that focus on the implementation and application of the watermarking algorithm to the text generated by the models.

- **plots.py**: A script to generate plots for visualizing the results of the analysis.

- **run_analysis.py**: A main script to execute the entire analysis, including quality metrics evaluation and text classification accuracy testing.



## Brief Overview

The core goal of this project is to study the effects of watermarking on generated text. Specifically, the repository aims to:

- **Evaluate Text Quality**: By calculating BLEU, ROUGE, and METEOR scores, the repository assesses how watermarking impacts the linguistic quality of the text generated by models like T5 and Flan-T5.

- **Analyze Classification Accuracy**: The repository investigates whether watermarking influences the accuracy of text classification tasks by comparing results from models before and after watermarking.

- **Provide Tools and Scripts**: The repository includes all necessary scripts and tools for replicating the analysis, making it a useful resource for researchers and practitioners interested in text watermarking and its implications.

This analysis is crucial for understanding the trade-offs between the security benefits of watermarking and its potential impact on text quality and the performance of downstream tasks like classification. The findings from this study could inform future research and development of watermarking techniques that minimize negative effects while preserving the integrity of the watermark.

